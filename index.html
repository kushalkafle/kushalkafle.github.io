<!DOCTYPE html>
<html lang="en">
<head>
  <title>Kushal Kafle</title>
     <description>Kushal Kafle Personal Homepage</description>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
  <link href="http://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet" type="text/css">
  <link href="http://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
  <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
  <style>
  h2 {
      font-size: 24px;
      text-transform: uppercase;
      color: #303030;
      font-weight: 600;
      margin-bottom: 5px;
  }
  h4 {
      font-size: 19px;
      line-height: 1.375em;
      color: #303030;
      font-weight: 400;
      margin-bottom: 15px;
  }
.h4-small {
      font-size: 14px;
      line-height: 1.375em;
      color: #303030;
      font-weight: 400;
      margin-bottom: 10px;
  } 
  .jumbotron {
      background-color: #2c3e50;
      color: #fff;
      padding: 100px 25px;
      font-family: Montserrat, sans-serif;
  }
  .container-fluid {
      padding: 60px 50px;
  }
  .bg-grey {
      background-color: #f6f6f6;
  }
  .logo-small {
      color: #2c3e50;
      font-size: 50px;
  }
  .logo {
      color: #2c3e50;
      font-size: 200px;
  }
  .thumbnail {
      padding: 0 0 15px 0;
      border: none;
      border-radius: 0;
  }
  .thumbnail img {
      width: 100%;
      height: 100%;
      margin-bottom: 10px;
  }
  .carousel-control.right, .carousel-control.left {
      background-image: none;
      color: #2c3e50;
  }
  .carousel-indicators li {
      border-color: #2c3e50;
  }
  .carousel-indicators li.active {
      background-color: #2c3e50;
  }
  .item h4 {
      font-size: 19px;
      line-height: 1.375em;
      font-weight: 400;
      font-style: italic;
      margin: 70px 0;
  }
  .item span {
      font-style: normal;
  }
  .panel {
      border: 1px solid #2c3e50; 
      border-radius:0 !important;
      transition: box-shadow 0.5s;
  }
  .panel:hover {
      box-shadow: 5px 0px 40px rgba(0,0,0, .2);
  }
  .panel-footer .btn:hover {
      border: 1px solid #2c3e50;
      background-color: #fff !important;
      color: #2c3e50;
  }
  .panel-heading {
      color: #fff !important;
      background-color: #2c3e50 !important;
      padding: 25px;
      border-bottom: 1px solid transparent;
      border-top-left-radius: 0px;
      border-top-right-radius: 0px;
      border-bottom-left-radius: 0px;
      border-bottom-right-radius: 0px;
  }
  .panel-footer {
      background-color: white !important;
  }
  .panel-footer h3 {
      font-size: 32px;
  }
  .panel-footer h4 {
      color: #aaa;
      font-size: 14px;
  }
  .panel-footer .btn {
      margin: 15px 0;
      background-color: #2c3e50;
      color: #fff;
  }
  .navbar {
      margin-bottom: 0;
      background-color: #2c3e50;
      z-index: 9999;
      border: 0;
      font-size: 12px !important;
      line-height: 1.42857143 !important;
      letter-spacing: 4px;
      border-radius: 0;
      font-family: Montserrat, sans-serif;
  }
  .navbar li a, .navbar .navbar-brand {
      color: #fff !important;
  }
  .navbar-nav li a:hover, .navbar-nav li.active a {
      color: #2c3e50 !important;
      background-color: #fff !important;
  }
  .navbar-default .navbar-toggle {
      border-color: transparent;
      color: #fff !important;
  }
  footer .glyphicon {
      font-size: 20px;
      margin-bottom: 20px;
      color: #2c3e50;
  }
  .slideanim {visibility:hidden;}
  .slide {
      animation-name: slide;
      -webkit-animation-name: slide;	
      animation-duration: 1s;	
      -webkit-animation-duration: 1s;
      visibility: visible;			
  }
  @keyframes slide {
    0% {
      opacity: 0;
      -webkit-transform: translateY(70%);
    } 
    100% {
      opacity: 1;
      -webkit-transform: translateY(0%);
    }	
  }
  @-webkit-keyframes slide {
    0% {
      opacity: 0;
      -webkit-transform: translateY(70%);
    } 
    100% {
      opacity: 1;
      -webkit-transform: translateY(0%);
    }
  }
  @media screen and (max-width: 768px) {
    .col-sm-4 {
      text-align: center;
      margin: 25px 0;
    }
    .btn-lg {
        width: 100%;
        margin-bottom: 35px;
    }
  }
  @media screen and (max-width: 480px) {
    .logo {
        font-size: 150px;
    }
  }
  .affix {
  padding:0px;
  -webkit-transition:padding 0.2s linear;
  -moz-transition:padding 0.2s linear;  
  -o-transition:padding 0.2s linear;         
  transition:padding 0.2s linear;  

}

.affix-top {
  padding-top:20px;
  padding-bottom:20px;
  -webkit-transition:padding 0.5s linear;
  -moz-transition:padding 0.5s linear;  
  -o-transition:padding 0.5s linear;         
  transition:padding 0.5s linear;  
}
      footer {
        background-color: #2c3e50;

}
  </style>
</head>
<body id="Home" data-spy="scroll" data-target=".navbar" data-offset="300">

<nav class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>                        
      </button>
      <a class="navbar-brand" href="#Home">Kushal Kafle</a>
    </div>
    <div class="collapse navbar-collapse" id="myNavbar">
      <ul class="nav navbar-nav navbar-right">
              <li data-toggle="collapse" data-target=".navbar-collapse.in"><a href="#home">HOME</a></li>

    <li data-toggle="collapse" data-target=".navbar-collapse.in"><a href="#about">ABOUT</a></li>
   <li data-toggle="collapse" data-target=".navbar-collapse.in"><a href="#news" >RECENT</a></li>
    <li data-toggle="collapse" data-target=".navbar-collapse.in"><a href="#research">RESEARCH</a></li>
        <li data-toggle="collapse" data-target=".navbar-collapse.in"><a href="KushalKafle_CV.pdf" target="_blank">C.V.</a></li>
<!--        <li><a href="#contact">CONTACT</a></li>-->
      </ul>
    </div>
  </div>
</nav>
<br>

<div class="jumbotron" id="home">
    <div class="row">
        <div class="text-center">
  <h1>Kushal Kafle<br><a href="https://www.linkedin.com/in/kushal-kafle-23247458" target="_blank">
            <img border="0" alt="Kushal Kafle Linkedin" src="linkedin-logo.png" width="64"></a> <a href="https://scholar.google.com/citations?user=M_iwxCQAAAAJ&hl=en" target="_blank">
            <img border="0" alt="Kushal Kafle Google Scholar" src="gs-logo.png" width="64"></a> </h1>
  Ph.D. Student at Center for Imaging Science, RIT <br>
      kk6055_AT_rit.edu
            
    </div>
    </div>
</div>

<!-- Container (About Section) -->
<div id="about" class="container-fluid">
  <div class="row">
    <div class="col-sm-6 col-sm-offset-2">
      <h2>About Me</h2><br>
        <h4>I am a fourth year Ph.D. student in <a href="http://www.cis.rit.edu" target="_blank">Chester F. Carlson Center for Imaging Science</a> at <a href="http://www.rit.edu" target="_blank">Rochester Institute of Technology</a>. I work at <a href="http://klab.cis.rit.edu" target="_blank">Machine and Neuromorphic Perception Laboratory, (a.k.a. klab)</a> which is directed by my advisor, Dr. <a href="http://chriskanan.com" target="_blank">Christopher Kanan.</a></h4>
      <h4>Much of my research is devoted to developing better image understanding models. I am interested in applying deep learning and natural language processing techniques to simultaneously understand and communicate about the images. </h4>
      </div>
      <div class="col-sm-2">
        <br><br>
      <span><img src="kushal.jpg" alt="Kushal Kafle"></span>
    </div>
  </div>
</div>
    <div id="news" class = "container-fluid bg-grey">
        <div class="row">
    <div class="col-sm-8 col-sm-offset-2">

        <h2>Timeline Events</h2><br>
     <h4><strong>Jul 2017:</strong> Our new paper "An Analysis of Visual Question Answering" was accepted to ICCV ! Also, <a href="https://arxiv.org/pdf/1703.09684.pdf" target="_blank">available on arXiv.</a></h4>
      <h4><strong>Jun 2017:</strong> Our short <a href="kafle2017inlg.pdf" target="_blank">paper</a> "Data Augmentation for Visual Question Answering" was accepted to INLG 2017.
      <h4><strong>Jun 2017:</strong> Visual Question Answering (VQA) survey paper titled "Visual Question Answering: Datasets, Algorithms, and Future Challenges" was accepted to Computer Vision and Image Understanding Journal (CVIU). Also <a href="https://arxiv.org/abs/1610.01465">available on arXiv.</a></h4>
      <h4><strong>May 2017:</strong> Started working as Research Intern at Adobe Research.</h4>
    <h4><strong>May 2016:</strong> My application to <a href = "https://sites.google.com/site/deeplearningsummerschool2016/" target="_blank"> Deep Learning Summer School, 2016 </a>was accepted with scholarship.</h4>
    <h4><strong>Apr 2016:</strong> Launched <a href="http://askimage.org" target="_blank">online web demo</a> for Visual Question Answering </h4>
    <h4><strong>Mar 2016:</strong> Our Paper "Answer Type Prediction for Visual Question Answering" was <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Kafle_Answer-Type_Prediction_for_CVPR_2016_paper.html" target="_blank">accepted to CVPR, 2016 !</a> </h4>
    <h4><strong>Mar 2016:</strong> Our Amazon Web Services (AWS) grant proposal was accepted. Awarded $15K worth AWS credits.</h4>
      <h4><strong>Jul 2015:</strong> Started Working at Machine and Neuromorphic Perception Laboratory under Dr. Christopher Kanan </h4>
    <h4><strong>Jun 2015:</strong> Passed Ph.D. qualifying exam</h4>
    </div>
    </div>
        </div>


<!--<div id="research" class="container-fluid">
  <div class="row">
    
    <div class="col-sm-6 col-sm-offset-2">
      <h2>Research</h2>
        <h3><strong>Visual Question Answering</strong></h3>
        <h4>Visual Question Answering (VQA) is a new problem in computer vision and natural language processing in which an algorithm is given an image and a text-based question about the image, and the VQA algorithm must produce a text-based response to the question.<br>
        <br>
        VQA is specifically difficult given the dual requirement of (1) Implicitly performing object recognition, detection, scene and activity recognition etc. and (2) Understanding the Natural Language Question.<br>
        <br>
        Be.
    </h4>
      
    </div>
      <div class="col-sm-2">
      <div class="slideanim"> <a href="#publications" class="thumbnail"><img src="vqa.png" alt="OVERVIEW"> </a><br>
</div>
    </div>
  </div>
</div>-->

    <div id="research">
        <div class="container-fluid">
        <h2 class="text-center">Publications</h2><br>

    <div class="row">
    <div class="col-sm-3 col-sm-offset-2">
      <div class="slideanim"> <img src="vqa-tasks.png" class="thumbnail img-responsive" alt="Different Tasks in VQA"><br>
      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="https://arxiv.org/abs/1703.09684" target="_blank"><h4 class="mark"><strong>An Analysis of Visual Question Answering Algorithms</strong><br>
            <small><strong>Kushal Kafle</strong> and Christopher Kanan</small></h4></a>
      <h4 class="h4-small"> Analyzing and comparing different VQA algorithms is notoriously opaque and difficult. In this paper, we analyze existing VQA algorithms using a new dataset that contains over 1.6 million questions organized into 12 different categories including questions that are meaningless for a given image. We also propose new evaluation schemes that compensate for over-represented question-types and make it easier to study the strengths and weaknesses of algorithms. Our experiments establish how attention helps certain categories more than others,  determine which models work better than others, and explain how simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.
</h4>
 <h4 class="text-danger"><strong>The IEEE International Conference on Computer Vision (ICCV 2017)</strong><br> 
  <a href="https://arxiv.org/abs/1703.09684" target="_blank"><kbd>Paper</kbd></a> 
     <a href="projects/tdiuc.html"><kbd>Project</kbd></a><br>     
        </h4>
        
        </div>
        </div>
        
        
          
          <div class="row">
    <div class="col-sm-3 col-sm-offset-2">
      <div class="slideanim"> <img src="vqa-framework.png" alt="OVERVIEW" class="thumbnail img-responsive"><br>
      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="https://eventos.citius.usc.es/inlg2017/resources/final/1/1_Paper.pdf" target="_blank"><h4 class="mark"><strong>Data Augmentation for Visual Question Answering</strong><br>
            <small><strong>Kushal Kafle,</strong> Mohammed Yousefhussien and Christopher Kanan</small></h4></a>
      <h4 class="h4-small">In this short paper, we describe two simple means of producing new training data for visual question answering algorithms. Data augmentation using these methods show increased performance in both baseline and state of the art VQA algorithms, including pronounced increase in counting questions. which remain one of the most difficult problems in VQA. </h4>
        
 <h4 class="text-danger"><strong>International Natural Language Generation Conference (INLG 2017)</strong><br> 
  <a href="kafle2017inlg.pdf" target="_blank"> <kbd>Paper</kbd> </a>
        </h4>
        
        </div>
        </div>
          
          
          
            <div class="row">
    <div class="col-sm-3 col-sm-offset-2">
                <div class="slideanim"> <img src="VQA-classification.png" class="thumbnail img-responsive" alt="Common VQA Framework"><br>

      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="https://arxiv.org/abs/1610.01465" target="_blank"><h4 class="mark"><strong>Visual Question Answering: Datasets, Algorithms, and Future Challenges</strong><br>
            <small><strong>Kushal Kafle</strong> and Christopher Kanan</small></h4></a>
      <h4 class="h4-small"> Since the release of the first VQA dataset in 2014, additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research.
</h4>
 <h4 class="text-danger"><strong>Computer Vision and Image Understanding (CVIU)</strong><br>
  <a href="https://arxiv.org/abs/1610.01465" target="_blank"> <kbd>Paper</kbd> </a> </h4><br>
        
        </div>
        </div>
          
 
       
          
          
            <div class="row">
    <div class="col-sm-3 col-sm-offset-2">
      <div class="slideanim"> <img src="answertype.png" alt="OVERVIEW" class="thumbnail img-responsive"><br>
      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="http://ieeexplore.ieee.org/document/7780907/" target="_blank"><h4 class="mark"><strong>Answer-Type Prediction for Visual Question Answering</strong><br>
            <small><strong>Kushal Kafle</strong> and Christopher Kanan</small></h4></a>
      <h4 class="h4-small">In this paper, we build a system capable of answering open-ended text-based questions about images, which is known as Visual Question Answering (VQA). Our approach's key insight is that we can predict the form of the answer from the question. We formulate our solution in a Bayesian framework. When our approach is combined with a discriminative model, the combined model achieves state-of-the-art results (at the time of publication) on four benchmark datasets for open-ended VQA: DAQUAR, COCO-QA, The VQA Dataset, and Visual7W. </h4>
        
 <h4 class="text-danger"><strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)</strong><br> 
  <a href="http://ieeexplore.ieee.org/document/7780907/" target="_blank"> <kbd>Paper</kbd> </a>
        </h4>
        
        </div>
        </div>
    </div>
      </div>

        <!-- <div class="container-fluid bg-grey">
            <h2 class="text-center">Abstracts and posters</h2><br>       
            <div class="row">
    <div class="col-sm-3 col-sm-offset-2">
       <div class="slideanim"> <a href="#publications" class="thumbnail"><img src="vqa.png" alt="OVERVIEW"> </a><br></div>> 
    </div>
    
    <div class="col-sm-8 text-left col-sm-offset-2">
        <a href="https://arxiv.org/abs/1610.01465" target="_blank"><h4 class="mark"><strong>A Bayesian Model for Visual Question Answering</strong><br>
            <small> Christopher Kanan <strong>Kushal Kafle</strong></small></h4></a>
 <h4 class=""><em>16th Annual Meeting, Vision Sciences Society</em><br> </h4>
        
        </div>
        </div>
    </div>-->
    
    
<footer class="container-fluid text-center">
<script>
$(document).ready(function(){
  // Add smooth scrolling to all links in navbar + footer link
  $(".navbar a, footer a[href='#Home']").on('click', function(event) {

    // Prevent default anchor click behavior
    

    // Store hash
    var hash = this.hash;
    
    if (hash != "")
        {
            event.preventDefault();
        }
      
    // Using jQuery's animate() method to add smooth page scroll
    // The optional number (900) specifies the number of milliseconds it takes to scroll to the specified area
    $('html, body').stop().animate({
      scrollTop: $(hash).offset().top
    }, 900, function(){
   
      // Add hash (#) to URL when done scrolling (default click behavior)
      window.location.hash = hash;
    });
  });
  
  $(window).scroll(function() {
    $(".slideanim").each(function(){
      var pos = $(this).offset().top;

      var winTop = $(window).scrollTop();
        if (pos < winTop + 600) {
          $(this).addClass("slide");
        }
    });
  });
})
</script>    
    
<!--
    <a href="https://www.linkedin.com/in/kushal-kafle-23247458" target="_blank">
            <img border="0" alt="Kushal Kafle's Linkedin" src="linkedin.png" width="64"  onmouseover="this.src='linkedin_high.png'" onmouseout="this.src='linkedin.png'"></a></p>
-->
    
    <a href="#Home" title="To Top">
    <span class="glyphicon glyphicon-home" style="font-size: 40px; color:white"></span>
  </a> <hr>
    <p> © 2017 Kushal Kafle <br>

    </footer>
      


</body>
</html>
