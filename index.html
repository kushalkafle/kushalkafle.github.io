<!DOCTYPE html>
<html lang="en">
<head>
  <title>Kushal Kafle</title>
     <description>Kushal Kafle Personal Homepage</description>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Kushal Kafle">
    <meta property="og:url" content="https://kushalkafle.com/"/>
        <meta name='og:description' content='Kushal Kafle Personal Homepage'/>

        <meta property="og:image" content="https://kushalkafle.com/images/kushal.jpg"/>
        <meta property="og:image:url" content="https://kushalkafle.com/images/kushal.jpg"/>


		<meta name='twitter:title' content='Kushal Kafle'/>
        
        
        <meta name='twitter:image' content='https://kushalkafle.com/images/kushal.jpg'/>
        
        <meta name='twitter:creator' content='@kushalkafle'/>
        <meta name='twitter:site' content='@kushalkafle'/>
        <meta name='twitter:url' content='https://kushalkafle.com/'/>
        
        <meta name='twitter:description' content='Kushal Kafle Personal Homepage'/>


        <meta name="keywords" content="RIT, kushalkafle, Kushal Kafle, machine learning, AI, Artificial Intelligence, Computer VIsion, Deep learning, Neural Networks, Tigers">
	<!-- 	<link href=" data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfjAxYWBygMiVDgAAADWElEQVRIx5WVz2vcVRTFP+d95zuTyWQm045JY9qm1GKJRI1VWrWgFjdSaRGF0p2I4sa/wI1LFy4rKBSkG9eKQRdaf0CLFEXdCAXLpEmTajttTJPOTOb393tdzGRmksyk8W7e4t1z3jn3Xu4TrfAeYsjuEBPmKIY1dhiR5uFgkncoUzLHZb7n/xIghu0Ec0zgU+cKazslcK0zYcd1lRH2UmBahzYmeXjtxD4EWtMlPMYp4pMjpc0pUZzD4eFttSAAo2oJVQnN0xxpAdZ6XdgRTlPhJnn+IbvRXqsGAV5DFZZsjLIVFUFNvIdkL/A2Q0qSJ80vfNyTAKyuLHuJcktFBte9m+xlvUWcKdsjMcs3WurdBWgQkiQAMtwzwBHgjvM6+y1BCmORD/RruLUGTa+UbZIkJWbtrkDOQnkneMOmbA+7WNMsn3LR6EHAehkGVOdvm9YYl0Byr/CaTWmXDavGDT5hhjDoO0hokCFWSSnCXYpgL/KqTTNuUQpkuWBfqQcc3HrHrUgeiHOdLFX3nJ21x5XEo6gsnzHTG95tocYqPkOkGKfAYR0kRUUrLHPOfuwH31iDBYaJkqRhTxInpqo9xl/6kJ/6w+ka8SpRxoiwbPvYjVS0AzQ4zw9sAwdnHQUFSuSIKK4chfBZBnSDI5YytouOgoARRA24h/GSAq3i6yhnTDsisDq3qRAnQopnEFXy3KfGSe13OyEgtHlKRLjPiDnmARikQI3TeF5fgvaNpAN6BJ+MVfiW9+XzBA0KKpFmlUV7oIKAGgmSZFjgHFnOc408g0zIOMPD3oMIDOp4pFnRFf4VwTxfsJsECY7yFO/h+10woS0KTLIRyixSxg/w4Gv+JEGSUY1y0k4F2ysAqyqkjKjRgABWuMAaMaCquN7VhNsCdhuWbYMcS4ioWmuSn5mhTgFRIeTNjTaaXroINEAZkeZw02AIdb7kD8pEiPE7gxxrtCtmbRUdC7dYIsYq19u32CwXKRClyjF8nVKyU75mQrcFI8EAVy0g6lqdJbTvWGaAKqMMs8i+bs0hYfc/oUf1PEaAmGPNGi2pRfk6RAaPkKyi5FXoHqpugkDXyFmGz1mhZEHb4wK/6TY1PuIyN7Vim/dqpyke7mk3ubk5Dg85l2l+bJsn8j+qmELFeVcYCgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAxOS0wMy0yMlQxODowNzoxNS0wNDowMMahXBMAAAAldEVYdGRhdGU6bW9kaWZ5ADIwMTktMDMtMjJUMTg6MDc6MTUtMDQ6MDC3/OSvAAAAAElFTkSuQmCC" rel="icon" type="image/x-icon" />
		<link href=" data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfjAxYWBygMiVDgAAADWElEQVRIx5WVz2vcVRTFP+d95zuTyWQm045JY9qm1GKJRI1VWrWgFjdSaRGF0p2I4sa/wI1LFy4rKBSkG9eKQRdaf0CLFEXdCAXLpEmTajttTJPOTOb393tdzGRmksyk8W7e4t1z3jn3Xu4TrfAeYsjuEBPmKIY1dhiR5uFgkncoUzLHZb7n/xIghu0Ec0zgU+cKazslcK0zYcd1lRH2UmBahzYmeXjtxD4EWtMlPMYp4pMjpc0pUZzD4eFttSAAo2oJVQnN0xxpAdZ6XdgRTlPhJnn+IbvRXqsGAV5DFZZsjLIVFUFNvIdkL/A2Q0qSJ80vfNyTAKyuLHuJcktFBte9m+xlvUWcKdsjMcs3WurdBWgQkiQAMtwzwBHgjvM6+y1BCmORD/RruLUGTa+UbZIkJWbtrkDOQnkneMOmbA+7WNMsn3LR6EHAehkGVOdvm9YYl0Byr/CaTWmXDavGDT5hhjDoO0hokCFWSSnCXYpgL/KqTTNuUQpkuWBfqQcc3HrHrUgeiHOdLFX3nJ21x5XEo6gsnzHTG95tocYqPkOkGKfAYR0kRUUrLHPOfuwH31iDBYaJkqRhTxInpqo9xl/6kJ/6w+ka8SpRxoiwbPvYjVS0AzQ4zw9sAwdnHQUFSuSIKK4chfBZBnSDI5YytouOgoARRA24h/GSAq3i6yhnTDsisDq3qRAnQopnEFXy3KfGSe13OyEgtHlKRLjPiDnmARikQI3TeF5fgvaNpAN6BJ+MVfiW9+XzBA0KKpFmlUV7oIKAGgmSZFjgHFnOc408g0zIOMPD3oMIDOp4pFnRFf4VwTxfsJsECY7yFO/h+10woS0KTLIRyixSxg/w4Gv+JEGSUY1y0k4F2ysAqyqkjKjRgABWuMAaMaCquN7VhNsCdhuWbYMcS4ioWmuSn5mhTgFRIeTNjTaaXroINEAZkeZw02AIdb7kD8pEiPE7gxxrtCtmbRUdC7dYIsYq19u32CwXKRClyjF8nVKyU75mQrcFI8EAVy0g6lqdJbTvWGaAKqMMs8i+bs0hYfc/oUf1PEaAmGPNGi2pRfk6RAaPkKyi5FXoHqpugkDXyFmGz1mhZEHb4wK/6TY1PuIyN7Vim/dqpyke7mk3ubk5Dg85l2l+bJsn8j+qmELFeVcYCgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAxOS0wMy0yMlQxODowNzoxNS0wNDowMMahXBMAAAAldEVYdGRhdGU6bW9kaWZ5ADIwMTktMDMtMjJUMTg6MDc6MTUtMDQ6MDC3/OSvAAAAAElFTkSuQmCC" rel="shortcut icon" type="image/x-icon" />
 -->
		<link rel="icon" href="favicon.ico" type="image/x-icon" />
		<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />

   <link rel="me" href="//twitter.com/kushalkafle">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
  <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
  <style>
  h2 {
      font-size: 24px;
      text-transform: uppercase;
      color: #303030;
      font-weight: 600;
      margin-bottom: 5px;
  }
  h4 {
      font-size: 19px;
      line-height: 1.375em;
      color: #303030;
      font-weight: 400;
      margin-bottom: 15px;
  }
.h4-small {
      font-size: 14px;
      line-height: 1.375em;
      color: #303030;
      font-weight: 400;
      margin-bottom: 10px;
  } 
  .jumbotron {
      background-color: #2c3e50;
      color: #fff;
      padding: 100px 25px;
      font-family: Montserrat, sans-serif;
  }
  .container-fluid {
      padding: 60px 50px;
  }
  .bg-grey {
      background-color: #f6f6f6;
  }
  .logo-small {
      color: #2c3e50;
      font-size: 50px;
  }
  .logo {
      color: #2c3e50;
      font-size: 200px;
  }
  .thumbnail {
      padding: 0 0 15px 0;
      border: none;
      border-radius: 0;
  }
  .thumbnail img {
      width: 100%;
      height: 100%;
      margin-bottom: 10px;
  }
  .carousel-control.right, .carousel-control.left {
      background-image: none;
      color: #2c3e50;
  }
  .carousel-indicators li {
      border-color: #2c3e50;
  }
  .carousel-indicators li.active {
      background-color: #2c3e50;
  }
  .item h4 {
      font-size: 19px;
      line-height: 1.375em;
      font-weight: 400;
      font-style: italic;
      margin: 70px 0;
  }
  .item span {
      font-style: normal;
  }
  .panel {
      border: 1px solid #2c3e50; 
      border-radius:0 !important;
      transition: box-shadow 0.5s;
  }
  .panel:hover {
      box-shadow: 5px 0px 40px rgba(0,0,0, .2);
  }
  .panel-footer .btn:hover {
      border: 1px solid #2c3e50;
      background-color: #fff !important;
      color: #2c3e50;
  }
  .panel-heading {
      color: #fff !important;
      background-color: #2c3e50 !important;
      padding: 25px;
      border-bottom: 1px solid transparent;
      border-top-left-radius: 0px;
      border-top-right-radius: 0px;
      border-bottom-left-radius: 0px;
      border-bottom-right-radius: 0px;
  }
  .panel-footer {
      background-color: white !important;
  }
  .panel-footer h3 {
      font-size: 32px;
  }
  .panel-footer h4 {
      color: #aaa;
      font-size: 14px;
  }
  .panel-footer .btn {
      margin: 15px 0;
      background-color: #2c3e50;
      color: #fff;
  }
  .navbar {
      margin-bottom: 0;
      background-color: #2c3e50;
      z-index: 9999;
      border: 0;
      font-size: 12px !important;
      line-height: 1.42857143 !important;
      letter-spacing: 4px;
      border-radius: 0;
      font-family: Montserrat, sans-serif;
  }
  .navbar li a, .navbar .navbar-brand {
      color: #fff !important;
  }
  .navbar-nav li a:hover, .navbar-nav li.active a {
      color: #2c3e50 !important;
      background-color: #fff !important;
  }
  .navbar-default .navbar-toggle {
      border-color: transparent;
      color: #fff !important;
  }
  footer .glyphicon {
      font-size: 20px;
      margin-bottom: 20px;
      color: #2c3e50;
  }
  .slideanim {visibility:hidden;}
  .slide {
      animation-name: slide;
      -webkit-animation-name: slide;	
      animation-duration: 1s;	
      -webkit-animation-duration: 1s;
      visibility: visible;			
  }
  @keyframes slide {
    0% {
      opacity: 0;
      -webkit-transform: translateY(70%);
    } 
    100% {
      opacity: 1;
      -webkit-transform: translateY(0%);
    }	
  }
  @-webkit-keyframes slide {
    0% {
      opacity: 0;
      -webkit-transform: translateY(70%);
    } 
    100% {
      opacity: 1;
      -webkit-transform: translateY(0%);
    }
  }
  @media screen and (max-width: 768px) {
    .col-sm-4 {
      text-align: center;
      margin: 25px 0;
    }
    .btn-lg {
        width: 100%;
        margin-bottom: 35px;
    }
  }
  @media screen and (max-width: 480px) {
    .logo {
        font-size: 150px;
    }
  }
  .affix {
  padding:0px;
  -webkit-transition:padding 0.2s linear;
  -moz-transition:padding 0.2s linear;  
  -o-transition:padding 0.2s linear;         
  transition:padding 0.2s linear;  

}

.affix-top {
  padding-top:20px;
  padding-bottom:20px;
  -webkit-transition:padding 0.5s linear;
  -moz-transition:padding 0.5s linear;  
  -o-transition:padding 0.5s linear;         
  transition:padding 0.5s linear;  
}
      footer {
        background-color: #2c3e50;

}
  </style>
</head>
<body id="Home" data-spy="scroll" data-target=".navbar" data-offset="300">

<nav class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>                        
      </button>
      <a class="navbar-brand" href="#Home">Kushal Kafle</a>
    </div>
    <div class="collapse navbar-collapse" id="myNavbar">
      <ul class="nav navbar-nav navbar-right">
              <li data-toggle="collapse" data-target=".navbar-collapse.in"><a href="#home">HOME</a></li>
    <li data-toggle="collapse" data-target=".navbar-collapse.in"><a href="#about">ABOUT</a></li>
   <li data-toggle="collapse" data-target=".navbar-collapse.in"><a href="#news" >RECENT</a></li>
    <li data-toggle="collapse" data-target=".navbar-collapse.in"><a href="#research">RESEARCH</a></li>
    <li data-toggle="collapse" data-target=".navbar-collapse.in"><a href="#misc">MISC</a></li>

        <li data-toggle="collapse" data-target=".navbar-collapse.in"><a href="data/KushalKafle_CV.pdf" target="_blank">C.V.</a></li>
<!--        <li><a href="#contact">CONTACT</a></li>-->
      </ul>
    </div>
  </div>
</nav>
<br>

<div class="jumbotron" id="home">
    <div class="row">
        <div class="text-center">
  <h1>Kushal Kafle<br><a href="https://www.linkedin.com/in/kushal-kafle-23247458" target="_blank">
            <img border="0" alt="Kushal Kafle Linkedin" src="images/linkedin-logo.png" width="64"></a>
	  <a href="https://scholar.google.com/citations?user=M_iwxCQAAAAJ&hl=en" target="_blank">
           <img border="0" alt="Kushal Kafle Google Scholar" src="images/gs-logo.png" width="64"></a>
  
	  <a href="https://twitter.com/kushalkafle" target="_blank">
           <img border="0" alt="Kushal Kafle Twitter" src="images/twitter-logo.png" width="64"></a>
  </h1>
  Ph.D. Candidate at Center for Imaging Science, RIT <br>
      kk6055 (AT) rit.edu
            
    </div>
    </div>
</div>

<!-- Container (About Section) -->
<div id="about" class="container-fluid">
  <div class="row">
    <div class="col-sm-6 col-sm-offset-2">
      <h2>About Me</h2><br>
        <h4>I am a Ph.D. candidate in <a href="http://www.cis.rit.edu" target="_blank">Chester F. Carlson Center for Imaging Science</a> at <a href="http://www.rit.edu" target="_blank">Rochester Institute of Technology</a>. I work at <a href="http://klab.cis.rit.edu" target="_blank">Machine and Neuromorphic Perception Laboratory, (a.k.a. klab)</a> which is directed by my advisor, Dr. <a href="http://chriskanan.com" target="_blank">Christopher Kanan.</a></h4>
      <h4>The overarching goal of my research is to develop develop robust vision and language
		  models. Along the way, I have published about both novel data and algorithms for VQA (CVPR
		  2016, CVPR 2018, CVPR 2019) and the analysis of robustness of the VQA algorithms in presence of
		  dataset bias (ICCV 2017, CVIU 2017). I am currently exploring models that
		  demonstrate good understanding of visual content and are right for the right reasons.
        </h4>

<!--         <h4>I am married to <a href="http://jwaladhamala.com" target="_blank">Jwala Dhamala</a>, who
			is also a Ph.D. student in RIT and does fascinating <a
				href="https://scholar.google.com/citations?user=1bUxjvoAAAAJ&hl=en"
				target="_blank">research</a> in machine learning and computational biomedicine.
        </h4>
 -->      </div>
      <div class="col-sm-2">
        <br><br>
      <span><img src="images/kushal.jpg" alt="Kushal Kafle"></span>
    </div>
  </div>
</div>
    <div id="news" class = "container-fluid bg-grey">
        <div class="row">
    <div class="col-sm-8 col-sm-offset-2">

        <h2>Timeline Events</h2><br>
                                     <h4><strong>Sept 2019:</strong> Our <a href='https://arxiv.org/abs/1908.01801' target="_blank">paper</a> describing a novel state-of-the-art chart question answering algorithm was accepted to appear at WACV.</h4>

                             <h4><strong>Mar 2019:</strong> I will be working as a research intern at Microsoft Research, Redmond this summer.</h4>
                             <h4><strong>Mar 2019:</strong> Our <a href='https://arxiv.org/abs/1903.00366' target="_blank">new paper</a> "Answer Them All! Toward Universal Visual Question Answering Models" was accepted to CVPR 2019! </h4>

	                     <h4><strong>Jan 2019:</strong> I am co-organizing second edition of "Workshop on Shortcomings in
						 vision and language (SiVL)" in NAACL, 2019.</h4>
                             <h4><strong>Nov 2018:</strong> I successfully defended my thesis proposal aka, advancement to candidacy.</h4>
                            <h4><strong>Nov 2018:</strong> Our <a href='https://arxiv.org/abs/1810.12440' target="_blank">new paper</a> "TallyQA: Answering Complex Counting Questions" was accepted to AAAI 2019!</h4>

                     <h4><strong>July 2018:</strong> I am co-organizing "Workshop on Shortcomings in
						 vision and language (SiVL)" in ECCV, 2018.</h4>
                     <h4><strong>May 2018:</strong> I was recognized as an outstanding reviewer for CVPR 2018! </h4>
             <h4><strong>Feb 2018:</strong> Our <a href="https://arxiv.org/abs/1801.08163" target="_blank"> new paper </a> "DVQA: Understanding Data Visualizations via Question Answering" was accepted to CVPR 2018! </h4>
     <h4><strong>Jul 2017:</strong> Our new paper "An Analysis of Visual Question Answering" was accepted to ICCV 2017! Also, <a href="https://arxiv.org/pdf/1703.09684.pdf" target="_blank">available on arXiv.</a></h4>
        <h4><strong>Jun 2017:</strong> Our short <a href="kafle2017inlg.pdf" target="_blank">paper</a> "Data Augmentation for Visual Question Answering" was accepted to INLG 2017.</h4>
      <h4><strong>Jun 2017:</strong> Visual Question Answering (VQA) survey paper titled "Visual Question Answering: Datasets, Algorithms, and Future Challenges" was accepted to Computer Vision and Image Understanding Journal (CVIU). Also <a href="https://arxiv.org/abs/1610.01465">available on arXiv.</a></h4>
      <h4><strong>May 2017:</strong> Started working as Research Intern at Adobe Research.</h4>
    <h4><strong>May 2016:</strong> My application to <a href = "https://sites.google.com/site/deeplearningsummerschool2016/" target="_blank"> Deep Learning Summer School, 2016 </a>was accepted with scholarship.</h4>
    <h4><strong>Apr 2016:</strong> Launched <a href="http://askimage.org" target="_blank">online web demo</a> for Visual Question Answering. (<small class="text-danger mark"> New!: </small> Added demo for DVQA as well ! ) </h4>
    <h4><strong>Mar 2016:</strong> Our Paper "Answer Type Prediction for Visual Question Answering" was <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Kafle_Answer-Type_Prediction_for_CVPR_2016_paper.html" target="_blank">accepted to CVPR, 2016 !</a> </h4>
    <h4><strong>Mar 2016:</strong> Our Amazon Web Services (AWS) grant proposal was accepted. Awarded $15K worth AWS credits.</h4>
      <h4><strong>Jul 2015:</strong> Started Working at Machine and Neuromorphic Perception Laboratory under Dr. Christopher Kanan </h4>
    </div>
    </div>
        </div>


<!--<div id="research" class="container-fluid">
  <div class="row">
    
    <div class="col-sm-6 col-sm-offset-2">
      <h2>Research</h2>
        <h3><strong>Visual Question Answering</strong></h3>
        <h4>Visual Question Answering (VQA) is a new problem in computer vision and natural language processing in which an algorithm is given an image and a text-based question about the image, and the VQA algorithm must produce a text-based response to the question.<br>
        <br>
        VQA is specifically difficult given the dual requirement of (1) Implicitly performing object recognition, detection, scene and activity recognition etc. and (2) Understanding the Natural Language Question.<br>
        <br>
        Be.
    </h4>
      
    </div>
      <div class="col-sm-2">
      <div class="slideanim"> <a href="#publications" class="thumbnail"><img src="vqa.png" alt="OVERVIEW"> </a><br>
</div>
    </div>
  </div>
</div>-->

    <div id="research">
        <div class="container-fluid">
        <h2 class="text-center">Publications</h2><br>









                       <hr/>









 <div class="row">
    <div class="col-sm-3 col-sm-offset-2">
      <div class="slideanim"> <img src="images/remind.png" class="thumbnail img-responsive" alt="REMIND Your Neural Network to Prevent Catastrophic Forgetting"><br>
      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="https://arxiv.org/abs/1910.02509" target="_blank"><h4 class="mark"><strong>REMIND Your Neural Network to Prevent Catastrophic Forgetting</strong><br>
            <small>Tyler L. Hayes*, <strong>Kushal Kafle*</strong>, Robik Shrestha*, Manoj Acharya, and Christopher Kanan  <br> * denotes equal contribution </small></h4></a>
      <h4 class="h4-small"> 
In lifelong machine learning, an agent must be incrementally updated with new knowledge, instead of having distinct train and deployment phases.  For incrementally training convolutional neural network models, prior work has enabled replay by storing raw images, but this is memory intensive and not ideal for embedded agents. Here, we propose REMIND, a tensor quantization approach that enables efficient replay with tensors. Unlike other methods, REMIND is trained in a streaming manner, meaning it learns one example at a time rather than in large batches containing multiple classes. Our approach achieves state-of-the-art results for incremental class learning on the ImageNet-1K dataset. We demonstrate REMIND's generality by pioneering multi-modal incremental learning for visual question answering (VQA), which cannot be readily done with comparison models.</h4>
 <h4 class="text-danger"><strong>Under Review, available on arXiv (2019) </strong><br> 
  <a href="https://arxiv.org/abs/1910.02509" target="_blank"><kbd>Paper</kbd></a> 
     <kbd data-toggle="collapse" data-target="#remind2020">Bibtex</kbd>    
     <div id="remind2020" class="collapse">
    <pre><code>@article{hayes2019remind,
  title={REMIND Your Neural Network to Prevent Catastrophic Forgetting},
  author={Hayes, Tyler L and Kafle, Kushal and Shrestha, Robik and Acharya, Manoj and Kanan, Christopher},
  journal={arXiv preprint arXiv:1910.02509},
  year={2019}
}
</code></pre>
  </div>
        </h4><br>
        
        </div>
        </div>











 <div class="row">
    <div class="col-sm-3 col-sm-offset-2">
      <div class="slideanim"> <img src="images/prefil.png" class="thumbnail img-responsive" alt="Parallel Recurrent Fusion for Chart Question Answering"><br>
      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="https://arxiv.org/abs/1908.01801" target="_blank"><h4 class="mark"><strong>Answering Questions about Data Visualizations using Efficient Bimodal Fusion</strong><br>
            <small><strong>Kushal Kafle</strong>, Robik Shrestha, and Christopher Kanan</small></h4></a>
      <h4 class="h4-small"> 
Chart question answering (CQA) is a newly proposed visual question answering (VQA) task where an algorithm must answer questions about data visualizations, e.g. bar charts, pie charts, and line graphs. Here, we propose a novel CQA algorithm called parallel recurrent fusion of image and language (PReFIL). PReFIL first learns bimodal embeddings by fusing question and image features and then intelligently aggregates these learned embeddings to answer the given question. Despite its simplicity, PReFIL greatly surpasses state-of-the art systems and human baselines on both the FigureQA and DVQA datasets. Additionally, we demonstrate that PReFIL can be used to reconstruct tables by asking a series of questions about a chart.

</h4>
 <h4 class="text-danger"><strong> IEEE Winter Conference on Applications of Computer Vision (WACV 2020) </strong><br> 
  <a href="https://arxiv.org/abs/1908.01801" target="_blank"><kbd>Paper</kbd></a> 
     <kbd data-toggle="collapse" data-target="#wacv2020">Bibtex</kbd>    
     <div id="wacv2020" class="collapse">
    <pre><code>@inproceedings{kafle2020prefil,
title={Answering Questions about Data Visualizations using Efficient Bimodal Fusion},
  author={Kafle, Kushal and Shrestha, Robik and  Kanan, Christopher},
  booktitle={WACV},
  year={2020}
}
</code></pre>
  </div>
        </h4><br>
        
        </div>
        </div>











                        <hr/>
<div class="row">
    <div class="col-sm-3 col-sm-offset-2">
      <div class="slideanim"> <img src="images/challenges.png" class="thumbnail img-responsive" alt="Bias amplification in VQA"><br>
      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="https://arxiv.org/abs/1904.09317" target="_blank"><h4 class="mark"><strong>Challenges and Prospects in Vision and Language Research
</strong><br>
            <small><strong>Kushal Kafle</strong>, Robik Shrestha, and Christopher Kanan</small></h4></a>
      <h4 class="h4-small"> 
Language grounded image understanding tasks have often been proposed as a method for evaluating progress in artificial intelligence. Ideally, these tasks should test a plethora of capabilities that integrate computer vision, reasoning, and natural language understanding. However, rather than behaving as visual Turing tests, recent studies have demonstrated state-of-the-art systems are achieving good performance through flaws in datasets and evaluation procedures. We review the current state of affairs and outline a path forward.


</h4>
 <h4 class="text-danger"><strong>Frontiers in Artificial Intelligence - Language and Computation (Accepted, 2019)</strong><br> 
  <a href="https://arxiv.org/abs/1904.09317" target="_blank"><kbd>Paper</kbd></a> 
     <kbd data-toggle="collapse" data-target="#frontiers2019">Bibtex</kbd>    
     <div id="frontiers2019" class="collapse">
    <pre><code>@article{kafle2019challenges,
  title={Challenges and Prospects in Vision and Language Research},
  author={Kafle, Kushal and Shrestha, Robik and Kanan, Christopher},
  journal={arXiv preprint arXiv:1904.09317},
  year={2019}
}
</code></pre>
  </div>
        </h4><br>
        
        </div>
        </div>






                       <hr/>
 <div class="row">
    <div class="col-sm-3 col-sm-offset-2">
      <div class="slideanim"> <img src="images/ramen.png" class="thumbnail img-responsive" alt="RAMEN"><br>
      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="https://arxiv.org/abs/1903.00366" target="_blank"><h4 class="mark"><strong>Answer Them All! Toward Universal Visual Question Answering Models
</strong><br>
            <small>Robik Shrestha, <strong>Kushal Kafle</strong>, and Christopher Kanan</small></h4></a>
      <h4 class="h4-small"> 
      	   Visual Question Answering (VQA) research is  split into two camps: the first focuses on VQA datasets that require natural image understanding and the second focuses on synthetic datasets that test reasoning. A good VQA algorithm should be capable of both, but only a few VQA algorithms are tested in this manner. We compare five state-of-the-art VQA algorithms across eight VQA datasets covering both domains. To make the comparison fair, all of the models are standardized as much as possible, E.g., they use the same visual features, answer vocabularies, etc. We find that methods do not generalize across the two domains. To address this problem, we propose a new VQA algorithm that rivals or exceeds the state-of-the-art for both domains.


</h4>
 <h4 class="text-danger"><strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019)</strong><br> 
  <a href="https://arxiv.org/abs/1903.00366" target="_blank"><kbd>Paper</kbd></a> 
     <kbd>Code (Coming Soon)</kbd>
     <kbd data-toggle="collapse" data-target="#cvpr2019">Bibtex</kbd>    
     <div id="cvpr2019" class="collapse">
    <pre><code>@inproceedings{shrestha2019ramen,
title={Answer Them All! Toward Universal Visual Question Answering Models},
  author={Shrestha, Robik and Kafle, Kushal and Kanan, Christopher},
  booktitle={CVPR},
  year={2019}
    }
</code></pre>
  </div>
        </h4><br>
        
        </div>
        </div>








                        <hr/>
<div class="row">
    <div class="col-sm-3 col-sm-offset-2">
      <div class="slideanim"> <img src="images/tallyqa.png" class="thumbnail img-responsive" alt="TallyQA"><br>
      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="https://arxiv.org/abs/1810.12440" target="_blank"><h4 class="mark"><strong>TallyQA: Answering Complex Counting Questions
</strong><br>
            <small>Manoj Acharya, <strong>Kushal Kafle</strong>, and Christopher Kanan</small></h4></a>
      <h4 class="h4-small"> Most counting questions in visual question answering (VQA) datasets are simple and require no more than object detection. Here, we study algorithms for complex counting questions that involve relationships between objects, attribute identification, reasoning, and more. To do this, we created TallyQA, the world's largest dataset for open-ended counting. We propose a new algorithm for counting that uses relation networks with region proposals. Our method lets relation networks be efficiently used with high-resolution imagery. It yields state-of-the-art results compared to baseline and recent systems on both TallyQA and the HowMany-QA benchmark.


</h4>
 <h4 class="text-danger"><strong>Association for the Advancement of Artificial Intelligence (AAAI 2019)</strong><br> 
  <a href="https://arxiv.org/abs/1810.12440" target="_blank"><kbd>Paper</kbd></a> 
     <a href="https://www.manojacharya.com/tallyqa.html" target="_blank"><kbd>Project Page</kbd></a>
     <kbd data-toggle="collapse" data-target="#aaai2018">Bibtex</kbd>    
     <div id="aaai2018" class="collapse">
    <pre><code>@inproceedings{acharya2019tallyqa,
  title={TallyQA: Answering Complex Counting Questions},
  author={Acharya, Manoj and Kafle, Kushal and Kanan, Christopher},
  booktitle={AAAI},
  year={2019}
  }
</code></pre>
  </div>
        </h4><br>
        
        </div>
        </div>
       
               <hr/>
     
    <div class="row">
    <div class="col-sm-3 col-sm-offset-2">
      <div class="slideanim"> <img src="images/dvqa.png" class="thumbnail img-responsive" alt="DVQA"><br>
      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="https://arxiv.org/abs/1801.08163" target="_blank"><h4 class="mark"><strong>DVQA: Understanding Data Visualizations via Question Answering
</strong><br>
            <small><strong>Kushal Kafle</strong>, Brian Price, Scott Cohen, and Christopher Kanan</small></h4></a>
      <h4 class="h4-small"> Bar charts are an effective way for humans to convey information to each other, but today's algorithms cannot parse them. Existing methods fail when faced with minor variations in appearance. Here, we present DVQA, a dataset that tests many aspects of bar chart understanding in a question answering framework. Unlike visual question answering (VQA), DVQA requires processing words and answers that are unique to a particular bar chart. State-of-the-art VQA algorithms perform poorly on DVQA, and we propose two strong baselines that perform considerably better. DVQA also serves as an important proxy task for several critical AI abilities, such as attention, working memory, visual reasoning and an ability to handle dynamic and out-of-vocabulary(OOV) labels. 

</h4>
 <h4 class="text-danger"><strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2018)</strong><br> 
  <a href="https://arxiv.org/abs/1801.08163" target="_blank"><kbd>Paper</kbd></a> 
     <a href="projects/dvqa.html" target="_blank"><kbd>Project Page</kbd></a>
     <kbd data-toggle="collapse" data-target="#kafle-cvpr2018">Bibtex</kbd>    
     <div id="kafle-cvpr2018" class="collapse">
    <pre><code>@inproceedings{kafle2018dvqa,
  title={DVQA: Understanding Data Visualizations via Question Answering},
  author={Kafle, Kushal and Price, Brian and Cohen, Scott and Kanan, Christopher},
  booktitle={CVPR},
  year={2018}
}
</code></pre>
  </div>
        </h4><br>
        
        </div>
        </div>
        <hr/>
       <div class="row">
    <div class="col-sm-3 col-sm-offset-2">
      <div class="slideanim"> <img src="images/vqa-tasks.png" class="thumbnail img-responsive" alt="Different Tasks in VQA"><br>
      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="https://arxiv.org/abs/1703.09684" target="_blank"><h4 class="mark"><strong>An Analysis of Visual Question Answering Algorithms</strong><br>
            <small><strong>Kushal Kafle</strong> and Christopher Kanan</small></h4></a>
      <h4 class="h4-small"> Analyzing and comparing different VQA algorithms is notoriously opaque and difficult. In this paper, we analyze existing VQA algorithms using a new dataset that contains over 1.6 million questions organized into 12 different categories including questions that are meaningless for a given image. We also propose new evaluation schemes that compensate for over-represented question-types and make it easier to study the strengths and weaknesses of algorithms. Our experiments establish how attention helps certain categories more than others,  determine which models work better than others, and explain how simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.
</h4>
 <h4 class="text-danger"><strong>The IEEE International Conference on Computer Vision (ICCV 2017)</strong><br> 
  <a href="https://arxiv.org/abs/1703.09684" target="_blank"><kbd>Paper</kbd></a> 
     <a href="projects/tdiuc.html" target="_blank"><kbd>Project Page</kbd></a>
     <kbd data-toggle="collapse" data-target="#kafle-iccv2017">Bibtex</kbd>    
     <div id="kafle-iccv2017" class="collapse">
    <pre><code>@inproceedings{kafle2017analysis,
  title={An Analysis of Visual Question Answering Algorithms},
  author={Kafle, Kushal and Kanan, Christopher},
  booktitle={ICCV},
  year={2017}
}
</code></pre>
  </div>
        </h4><br>
        
        </div>
        </div>     
           <hr/>
          <div class="row">
    <div class="col-sm-3 col-sm-offset-2">
      <div class="slideanim"> <img src="images/vqa-framework.png" alt="OVERVIEW" class="thumbnail img-responsive"><br>
      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="http://www.aclweb.org/anthology/W17-3529" target="_blank"><h4 class="mark"><strong>Data Augmentation for Visual Question Answering</strong><br>
            <small><strong>Kushal Kafle,</strong> Mohammed Yousefhussien and Christopher Kanan</small></h4></a>
      <h4 class="h4-small">In this short paper, we describe two simple means of producing new training data for visual question answering algorithms. Data augmentation using these methods show increased performance in both baseline and state of the art VQA algorithms, including pronounced increase in counting questions. which remain one of the most difficult problems in VQA. </h4>
        
 <h4 class="text-danger"><strong>International Natural Language Generation Conference (INLG 2017)</strong><br> 
  <a href="http://www.aclweb.org/anthology/W17-3529" target="_blank"> <kbd>Paper</kbd> </a>
 <kbd data-toggle="collapse" data-target="#kafle-inlg2017">Bibtex</kbd>    
     <div id="kafle-inlg2017" class="collapse">
    <pre><code>@inproceedings{kafle2017data,
  title={Data Augmentation for Visual Question Answering},
  author={Kafle, Kushal and Yousefhussien, Mohammed and Kanan, Christopher}
  booktitle={INLG},
  year={2017}
}

</code></pre>
         </div>

        </h4><br>
        
        </div>
        </div>
           <hr/>
          
          
            <div class="row">
    <div class="col-sm-3 col-sm-offset-2">
                <div class="slideanim"> <img src="images/VQA-classification.png" class="thumbnail img-responsive" alt="Common VQA Framework"><br>

      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="https://arxiv.org/abs/1610.01465" target="_blank"><h4 class="mark"><strong>Visual Question Answering: Datasets, Algorithms, and Future Challenges</strong><br>
            <small><strong>Kushal Kafle</strong> and Christopher Kanan</small></h4></a>
      <h4 class="h4-small"> Since the release of the first VQA dataset in 2014, additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research.
</h4>
 <h4 class="text-danger"><strong>Computer Vision and Image Understanding (CVIU)</strong><br>
  <a href="https://arxiv.org/abs/1610.01465" target="_blank"> <kbd>Paper</kbd> </a> 
     <kbd data-toggle="collapse" data-target="#kafle-cviu2017">Bibtex</kbd>    
     <div id="kafle-cviu2017" class="collapse">
    <pre><code>@article{kafle2017visual,
  title={Visual question answering: Datasets, algorithms, and future challenges},
  author={Kafle, Kushal and Kanan, Christopher},
  journal={Computer Vision and Image Understanding},
  year={2017}
}
</code></pre>
         </div>
        </h4><br>
        </div>
        </div>
           <hr/>
          
            <div class="row">
    <div class="col-sm-3 col-sm-offset-2">
      <div class="slideanim"> <img src="images/answertype.png" alt="OVERVIEW" class="thumbnail img-responsive"><br>
      </div>
    </div>
    
    <div class="col-sm-5 text-left">
        <a href="http://ieeexplore.ieee.org/document/7780907/" target="_blank"><h4 class="mark"><strong>Answer-Type Prediction for Visual Question Answering</strong><br>
            <small><strong>Kushal Kafle</strong> and Christopher Kanan</small></h4></a>
      <h4 class="h4-small">In this paper, we build a system capable of answering open-ended text-based questions about images, which is known as Visual Question Answering (VQA). Our approach's key insight is that we can predict the form of the answer from the question. We formulate our solution in a Bayesian framework. When our approach is combined with a discriminative model, the combined model achieves state-of-the-art results (at the time of publication) on four benchmark datasets for open-ended VQA: DAQUAR, COCO-QA, The VQA Dataset, and Visual7W. </h4>
        
 <h4 class="text-danger"><strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016)</strong><br> 
  <a href="http://ieeexplore.ieee.org/document/7780907/" target="_blank"> <kbd>Paper</kbd> </a>
      <kbd data-toggle="collapse" data-target="#kafle-cvpr2016">Bibtex</kbd>    
     <div id="kafle-cvpr2016" class="collapse">
    <pre><code>@inproceedings{kafle2016answer,
  title={Answer-type prediction for visual question answering},
  author={Kafle, Kushal and Kanan, Christopher},
  booktitle={CVPR},
  year={2016}
}
</code></pre>
         </div>
        </h4><br>
        
        </div>
        </div>
    </div>
      </div>

         <div id= "misc" class="container-fluid bg-grey">
            <h2 class="text-center">MISC</h2><br>       
            <div class="row">
    
    <div class="col-sm-8 text-left col-sm-offset-2">
             <h4><strong>I have an Erdos number of four!</strong> <br> 
             	What follows is some long-winded collaboration lineage hunting, but you can't deny the facts! I have an Erdos number of 4. <br>
             	 <a href="#">Kushal Kafle</a> -> <a href="http://www.chriskanan.com" target="_blank">Christopher Kanan</a> -> <a href="http://www.cis.upenn.edu/~kostas/" target="_blank">Kostas Danilidis</a>  -> <a href="https://kam.mff.cuni.cz/~valtr/" target="_blank">Pavel Valtr</a>  -> <a href="https://en.wikipedia.org/wiki/Paul_Erd%C5%91s" target="_blank">Paul Erdos</a> <br>
              Here's the <a href="https://www.csauthors.net/distance/kushal-kafle/paul-erdos"> link</a>.
                <h4><strong>I have some impressive academic lineage !</strong> <br>
             	<a href="https://en.wikipedia.org/wiki/Norbert_Wiener" target="_blank">Norbert Weiner</a> is my grand-grand-advisor. <a href="https://en.wikipedia.org/wiki/David_Hilbert" target="_blank">David Hilbert</a> and <a href="https://en.wikipedia.org/wiki/Bertrand_Russell" target="_blank">Bertrand Russell</a> are my grand-grand-grand-advisors. It only gets better from there! If you follow David Hilbert, you eventually encounter some really big-shot names:
             	<a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace" target="_blank">Laplace</a>, 
             	<a href="https://en.wikipedia.org/wiki/Joseph_Fourier" target="_blank">Fourier</a>,
             	<a href="https://en.wikipedia.org/wiki/Sim%C3%A9on_Denis_Poisson" target="_blank">Poisson</a>, 
             	<a href="https://en.wikipedia.org/wiki/Joseph-Louis_Lagrange" target="_blank">Lagrange</a>, 
             	<a href="https://en.wikipedia.org/wiki/Peter_Gustav_Lejeune_Dirichlet" target="_blank">Dirichlet</a>. 

             	Talk about standing on the shoulder of giants! Here's the <a href="https://neurotree.org/neurotree/tree.php?pid=754712"> link</a>. 

             

        
        </div>
        </div>
    </div>
    
    
<footer class="container-fluid text-center">
<script>
$(document).ready(function(){
  // Add smooth scrolling to all links in navbar + footer link
  $(".navbar a, footer a[href='#Home']").on('click', function(event) {

    // Prevent default anchor click behavior
    

    // Store hash
    var hash = this.hash;
    
    if (hash != "")
        {
            event.preventDefault();
        }
      
    // Using jQuery's animate() method to add smooth page scroll
    // The optional number (900) specifies the number of milliseconds it takes to scroll to the specified area
    $('html, body').stop().animate({
      scrollTop: $(hash).offset().top
    }, 900, function(){
   
      // Add hash (#) to URL when done scrolling (default click behavior)
      window.location.hash = hash;
    });
  });
  
  $(window).scroll(function() {
    $(".slideanim").each(function(){
      var pos = $(this).offset().top;

      var winTop = $(window).scrollTop();
        if (pos < winTop + 600) {
          $(this).addClass("slide");
        }
    });
  });
})
</script>    
    
    <a href="#Home" title="To Top">
    <span class="glyphicon glyphicon-home" style="font-size: 40px; color:white"></span>
  </a> <hr>
    </footer>
      


</body>
</html>
